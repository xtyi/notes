# Torch Compiler IR

https://pytorch.org/docs/main/torch.compiler_ir.html

https://pytorch.org/executorch/0.4/ir-ops-set-definition.html

https://dev-discuss.pytorch.org/t/defining-the-core-aten-opset/1464

https://pytorch.org/tutorials/intermediate/torch_export_nightly_tutorial.html#decompositions


IR == OpSet

ATen IR > Functional Aten IR(fx graph, before decomposition) > Core Aten IR(fx graph, after decomposition) > Prim IR

- Aten IR: 所有的 torch aten op
- Functional Aten IR: aten op 中的纯函数风格的 op
- Core Aten IR: Functional Aten IR 中粒度更细的 op, 可以由 functional aten op 分解得到, 但是不会进一步分解出显式的 type promotion 和 broadcasting op
- Prim IR: 新设计的一套 IR，粒度最细的 op

PyTorch 2.0 offers two set of IRs for backends to interface with: Core Aten IR and Prims IR.

## Functional Aten IR

functional aten ir: 2000+

在 native_functions.yaml 文件中记录

## Core Aten IR

Core aten ops is the core subset of aten operators that can be used to compose other operators.

Core aten IR is fully functional, and there is no inplace or _out variants in this opset.

In contrast to Prims IR, core aten ops reuses the existing aten ops in "native_functions.yaml", and it doesn’t further decompose ops into explicit type promotion and broadcasting ops.

This opset is designed to serve as the functional IR to interface with backends.

> This opset is still under active development, more ops will be added in the future.

目前有 187 个

aten.add.Scalar
aten.add.Tensor

aten.sub.Scalar
aten.sub.Tensor

aten.mul.Scalar
aten.mul.Tensor

aten.div.Scalar
aten.div.Scalar_mode
aten.div.Tensor
aten.div.Tensor_mode

aten.any
aten.any.dim
aten.any.dimss

aten.acos
aten.acosh
aten.asin
aten.asinh
aten.atan
aten.atan2
aten.atan2.out
aten.atanh

aten.amax
aten.amin

aten._adaptive_avg_pool2d(torch-mlir)
aten._adaptive_avg_pool2d_backward
aten._adaptive_avg_pool3d
aten._cdist_forward
aten._embedding_bag
aten._local_scalar_dense
aten._log_softmax(torch-mlir)
aten._native_batch_norm_legit(torch-mlir)
aten._native_batch_norm_legit.no_stats(torch-mlir)
aten._native_batch_norm_legit_no_training(torch-mlir)
aten._pdist_forward
aten._softmax
aten._to_copy(torch-mlir)
aten.abs
aten.adaptive_avg_pool1d
aten.addmm(torch-mlir)
aten.alias

aten.arange.start_step
aten.argmax
aten.argmin
aten.as_strided

aten.avg_pool1d
aten.avg_pool2d
aten.avg_pool2d_backward
aten.avg_pool3d
aten.bitwise_and.Scalar
aten.bitwise_and.Tensor
aten.bitwise_not
aten.bitwise_or.Scalar
aten.bitwise_or.Tensor
aten.bitwise_xor.Scalar
aten.bitwise_xor.Tensor
aten.bmm
aten.cat
aten.ceil
aten.clamp
aten.clamp.Tensor
aten.clone
aten.col2im
aten.constant_pad_nd
aten.convolution
aten.convolution_backward
aten.copy
aten.cos
aten.cosh
aten.cumsum
aten.diagonal
aten.embedding
aten.embedding_dense_backward (mlir-torch)
aten.empty.memory_format
aten.empty_strided
aten.eq.Scalar
aten.eq.Tensor
aten.erf
aten.exp
aten.expand
aten.expm1
aten.fill.Scalar
aten.flip
aten.floor
aten.fmod.Scalar
aten.fmod.Tensor
aten.full(torch-mlir)
aten.gather
aten.ge.Scalar
aten.ge.Tensor
aten.gelu
aten.grid_sampler_2d(torch-mlir)
aten.gt.Scalar
aten.gt.Tensor
aten.hardtanh
aten.index.Tensor
aten.index_put
aten.index_select
aten.isinf
aten.isnan
aten.le.Scalar
aten.le.Tensor
aten.leaky_relu
aten.log
aten.log10
aten.log1p
aten.log2
aten.logical_and
aten.logical_not
aten.logical_or
aten.logical_xor
aten.lt.Scalar
aten.lt.Tensor
aten.max.dim
aten.max_pool2d_with_indices
aten.max_pool2d_with_indices_backward
aten.max_pool3d_with_indices
aten.maximum
aten.mean
aten.mean.dim
aten.min.dim
aten.minimum
aten.mm

aten.native_dropout
aten.native_group_norm(torch-mlir)
aten.native_group_norm_backward
aten.native_layer_norm(torch-mlir)
aten.native_layer_norm_backward(torch-mlir)
aten.ne.Scalar
aten.ne.Tensor
aten.neg
aten.nonzero
aten.permute
aten.pow.Scalar
aten.pow.Tensor_Scalar
aten.pow.Tensor_Tensor
aten.prod
aten.prod.dim_int
aten.rand
aten.randn
aten.randperm
aten.reciprocal
aten.reflection_pad1d
aten.reflection_pad2d
aten.reflection_pad3d
aten.relu
aten.remainder.Scalar
aten.remainder.Tensor
aten.repeat
aten.replication_pad2d
aten.replication_pad3d
aten.resize_
aten.round
aten.rsqrt
aten.scalar_tensor
aten.scatter.src
aten.scatter.value
aten.scatter_add
aten.scatter_reduce.two
aten.select.int
aten.select_scatter
aten.sigmoid
aten.sign
aten.sin
aten.sinh
aten.slice.Tensor
aten.slice_scatter
aten.sort
aten.split_with_sizes(torch-mlir)
aten.sqrt
aten.squeeze.dim
aten.squeeze.dims(torch-mlir)
aten.sum.dim_IntList
aten.sym_numel
aten.sym_size.int
aten.sym_storage_offset
aten.sym_stride.int
aten.tan
aten.tanh
aten.topk
aten.trunc
aten.unsqueeze
aten.upsample_bilinear2d.vec(torch-mlir)
aten.upsample_nearest2d.vec
aten.var.correction
aten.var.dim
aten.view
aten.where.self

## Prims IR

Prims IR is a set of primitive operators that can be used to compose other operators. Prims IR is a lower level opset than core aten IR, and it further decomposes ops into explicit type promotion and broadcasting ops: `prims.convert_element_type` and `prims.broadcast_in_dim`. This opset is designed to interface with compiler backends.

> This opset is still under active development, more ops will be added in the future.

Prims IR 不是 Aten Op 的子集，而是设计的一套新的 IR

Prims IR 比 Core Aten IR 更底层


Prims op 现在有 123 个

### Core

prims.broadcast_in_dim
prims.convert_element_type


### Binary

prims.add
prims.sub
prims.mul
prims.div

prims.gcd

### Unary

prims.acos
prims.acosh
prims.asin
prims.asinh
prims.atan
prims.atanh
prims.cos
prims.cosh
prims.tan
prims.tanh
prims.sin
prims.sinh

prims.abs
prims.neg

prims.log
prims.log1p
prims.log2
prims.log10

### Compare

prims.maximum
prims.minimum
prims.le
prims.lt
prims.ge
prims.gt

### Unsorted

prims.bessel_i0
prims.bessel_i0e
prims.bessel_i1
prims.bessel_i1e
prims.bessel_j0
prims.bessel_j1
prims.bitwise_not
prims.cbrt
prims.ceil
prims.conj_physical
prims.digamma
prims.erf
prims.erf_inv
prims.erfc
prims.erfcx
prims.exp
prims.expm1
prims.exp2
prims.fill
prims.floor
prims.imag
prims.isfinite
prims.lgamma
prims.ndtri
prims.real
prims.reciprocal
prims.round
prims.sign
prims.signbit
prims.spherical_bessel_j0
prims.sqrt
prims.trunc
prims.atan2
prims.bitwise_and
prims.bitwise_or
prims.bitwise_xor
prims.eq
prims.fmax
prims.fmin
prims.fmod
prims.frexp
prims.hypot
prims.igamma
prims.igammac


prims.ne
prims.nextafter
prims.pow
prims.remainder
prims.rsqrt
prims.shift_left
prims.shift_right_arithmetic
prims.zeta
prims.as_strided
prims.collapse_view
prims.conj
prims.slice
prims.slice_in_dim
prims.split_dim
prims.squeeze
prims.transpose
prims.view_of
prims.view_of_dtype
prims.as_strided_scatter
prims.collapse
prims.cat
prims.reshape
prims.rev
prims.where
prims.clone
prims.device_put
prims.item
prims.maximum_value
prims.minimum_value
prims.copy_strided
prims.copy_to
prims.resize
prims.amax
prims.amin
prims.prod
prims.sum
prims.xor_sum
prims.var
prims.empty_strided
prims.empty_permuted
prims.scalar_tensor
prims.iota
prims.svd
prims.normal
prims.uniform
prims.fft_r2c
prims.fft_c2c
prims.fft_c2r
